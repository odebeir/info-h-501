{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "from scipy import ndimage,interpolate\n",
    "from scipy.ndimage.filters import convolve1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of usage of detectors\n",
    "\n",
    "> Bhat P., Zitnick L., Snavely N., Agarwala A., Agrawala M., Curless B., Cohen M., Kang S. Using Photographs to Enhance Videos of a Static Scene. Eurographics Symposium on Rendering (EGSR) 2007. [webpage](https://grail.cs.washington.edu/projects/videoenhancement/)\n",
    "\n",
    "[Video presentation](https://www.youtube.com/watch?v=1PktKqyRXIE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris corner detection\n",
    "\n",
    "> see also https://docs.opencv.org\n",
    "\n",
    "\n",
    "Let's look for corners. Since corners represents a variation in the gradient in the image, we will look for this \"variation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ima = np.zeros((50,50),dtype=np.uint8)\n",
    "ima[20:40,20:30] = 1\n",
    "\n",
    "def add_rect(ax,xy,c='r',rand=None): \n",
    "    rect = Rectangle(xy,7,7,facecolor=\"none\", edgecolor=c, alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    if rand:\n",
    "        for i in range(5):\n",
    "            rxy = (xy[0]+rand*np.random.rand()-rand/2,xy[1]+rand*np.random.rand()-rand/2)\n",
    "            add_rect(ax,rxy,c='g')\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(ima)\n",
    "add_rect(ax,(16,16),rand=3)\n",
    "add_rect(ax,(4,16))\n",
    "add_rect(ax,(16,30))\n",
    "plt.imshow(ima)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a grayscale image $I$. We are going to sweep a window $w(x,y)$ (with displacements u in the x direction and v in the y direction) $I$ and will calculate the variation of intensity.\n",
    "\n",
    "\\begin{equation} \n",
    "E(u,v) = \\sum _{x,y} w(x,y)[ I(x+u,y+v) - I(x,y)]^{2}\n",
    "\\end{equation} \n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "$w(x,y)$ is the window at position $(x,y)$\n",
    "\n",
    "$I(x,y)$ is the intensity at $(x,y)$\n",
    "\n",
    "$I(x+u,y+v)$ is the intensity at the moved window $(x+u,y+v)$\n",
    "    \n",
    "Since we are looking for windows with corners, we are looking for windows with a large variation in intensity. Hence, we have to maximize the equation above, specifically the term:\n",
    "\n",
    "$\\sum _{x,y}[ I(x+u,y+v) - I(x,y)]^{2}$\n",
    "\n",
    "Using Taylor expansion of $I(x+u,y+v)$\n",
    "\n",
    "\\begin{equation} \n",
    "E(u,v) \\approx  \\sum _{x,y} w(x,y) [ I(x,y) + u I_{x} + vI_{y} - I(x,y)]^{2}\n",
    "\\end{equation} \n",
    "\n",
    "where $I_{x}$ and $I_{y}$ are x and y image derivatives.\n",
    "\n",
    "### Image derivative\n",
    "\n",
    "> Image derivative can be approximated by discrete differentiation operators\n",
    "> \n",
    "\\begin{equation}\n",
    "f'(x) = \\lim_{h\\to0} \\frac{f(x+h) - f(x)}{h}\n",
    "\\end{equation}\n",
    ">\n",
    "> for images, the equivallent is given by the convolution ($*$) of the image $I$  by the Prewitt operator,\n",
    "> \n",
    "\\begin{equation}\n",
    "I_x = \\begin{bmatrix} \n",
    "+1 & 0 & -1 \\\\\n",
    "+1 & 0 & -1 \\\\\n",
    "+1 & 0 & -1\n",
    "\\end{bmatrix} * I\n",
    "\\end{equation}\n",
    ">\n",
    "\\begin{equation}\n",
    "\\mbox{and} \\quad \\quad I_y = \\begin{bmatrix} \n",
    "+1 & +1 & +1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{bmatrix} * I\n",
    "\\end{equation}\n",
    ">\n",
    "\n",
    "that gives:\n",
    "\n",
    "\\begin{equation} \n",
    "E(u,v) \\approx \\sum _{x,y} w(x,y) (u^{2}I_{x}^{2} + 2uvI_{x}I_{y} + v^{2}I_{y}^{2})\n",
    "\\end{equation} \n",
    "\n",
    "Which can be expressed in a matrix form as:\n",
    "\n",
    "\\begin{equation}\n",
    "E(u,v) \\approx \\begin{bmatrix} u & v \\end{bmatrix} \\left ( \\displaystyle \\sum_{x,y} w(x,y) \\begin{bmatrix} I_x^{2} & I_{x}I_{y} \\\\ I_xI_{y} & I_{y}^{2} \\end{bmatrix} \\right ) \\begin{bmatrix} u \\\\ v \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "E(u,v) \\approx \\begin{bmatrix} u & v \\end{bmatrix} M \\begin{bmatrix} u \\\\ v \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "M = \\displaystyle \\sum_{x,y} w(x,y) \\begin{bmatrix} I_x^{2} & I_{x}I_{y} \\\\ I_xI_{y} & I_{y}^{2} \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Harris suggested to use the following expression to speed up the corner detection instead of explicitely compute $\\lambda_{1,2}$\n",
    "\n",
    "\\begin{equation}\n",
    "R = det(M) - k(trace(M))^{2}\n",
    "\\label{eq:vector_ray}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "$det(M) = \\lambda_{1}\\lambda_{2}$\n",
    "\n",
    "$trace(M) = \\lambda_{1}+\\lambda_{2}$\n",
    "\n",
    "\n",
    "\n",
    "When $|R|$ is small, which happens when $\\lambda_1$ and $\\lambda_2$ are small, the region is flat.\n",
    "\n",
    "When $R<0$, which happens when $\\lambda_1 >> \\lambda_2$ or vice versa, the region is edge.\n",
    "\n",
    "When $R$ is large, which happens when $\\lambda_1$ and $\\lambda_2$ are large and $\\lambda_1 \\sim \\lambda_2$, the region is a corner.\n",
    "\n",
    "\n",
    "## Shi-Tomasi\n",
    "\n",
    "As an alternative to Harris' $R$, authors suggested to use:\n",
    "\n",
    "$ R = \\min(\\lambda_1, \\lambda_2) $\n",
    "\n",
    "> see Shi J and Tomasi C (1994). Good Features to Track. In: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pp 593–600.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpixel accuracy\n",
    "\n",
    "Application to corner detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage import data\n",
    "from skimage.feature import corner_harris, corner_subpix, corner_peaks\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from skimage.draw import ellipse\n",
    "\n",
    "# Sheared checkerboard\n",
    "tform = AffineTransform(scale=(1.3, 1.1), rotation=1, shear=0.7,\n",
    "                        translation=(110, 30))\n",
    "image = warp(data.checkerboard()[:90, :90], tform.inverse,\n",
    "             output_shape=(200, 310))\n",
    "# Ellipse\n",
    "rr, cc = ellipse(160, 175, 10, 100)\n",
    "image[rr, cc] = 1\n",
    "# Two squares\n",
    "image[30:80, 200:250] = 1\n",
    "image[80:130, 250:300] = 1\n",
    "\n",
    "coords = corner_peaks(corner_harris(image), min_distance=5, threshold_rel=0.02)\n",
    "coords_subpix = corner_subpix(image, coords, window_size=13)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10,10])\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "ax.plot(coords[:, 1], coords[:, 0], color='cyan', marker='o',\n",
    "        linestyle='None', markersize=6)\n",
    "ax.plot(coords_subpix[:, 1], coords_subpix[:, 0], '+r', markersize=15)\n",
    "ax.axis((0, 310, 200, 0))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harris corner detection suffers from a major issue, it is **not scale invariant**, *cornerness*, especially for a discrete image, depends on the image scale, this problem is taken into account with the two next detectors:\n",
    "\n",
    "* SIFT **Scale-Invariant Feature Transform**\n",
    "\n",
    "* SURF **Speeded Up Robust Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT (Scale-Invariant Feature Transform)\n",
    ">  Lowe, David G. (1999). \"Object recognition from local scale-invariant features\". Proceedings of the International Conference on Computer Vision. 2. pp. 1150–1157.\n",
    "\n",
    "* maxima and minima of the **DoG**, Difference of Gaussian, in the scale space\n",
    "\n",
    "* scale space obtained by filtering and sub-sampling\n",
    "\n",
    "* lower contrast points are eliminated (local maximum with 26 neib.)\n",
    "\n",
    "* local orientation signature using the histogram of oriented gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.zeros((40,125),dtype=np.uint8)\n",
    "rr, cc = ellipse(20, 60, 10, 50)\n",
    "image[rr, cc] = 1\n",
    "big_image = cv2.resize(image,(image.shape[1]*10,image.shape[0]*10),interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "coords = corner_peaks(corner_harris(image), min_distance=5, threshold_rel=0.04)\n",
    "big_coords = corner_peaks(corner_harris(big_image), min_distance=5, threshold_rel=0.04)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,figsize=[10,10])\n",
    "ax[0].imshow(image, cmap=plt.cm.gray)\n",
    "ax[0].plot(coords[:, 1], coords[:, 0], color='cyan', marker='o',\n",
    "        linestyle='None', markersize=6,alpha=.7)\n",
    "ax[1].imshow(big_image, cmap=plt.cm.gray)\n",
    "ax[1].plot(big_coords[:, 1], big_coords[:, 0], color='cyan', marker='o',\n",
    "        linestyle='None', markersize=6,alpha=.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramid images\n",
    "\n",
    "Multi-scale approach using pyramidal image decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.data import camera\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def build_gaussian_pyramid(ima,levelmax):\n",
    "    \"\"\"return a list of subsampled images (using gaussion pre-filter\n",
    "    \"\"\"\n",
    "    r = [ima]\n",
    "    current = ima\n",
    "    for level in range(levelmax):\n",
    "        lp = gaussian_filter(current,1.0)\n",
    "        sub = lp[::2,::2]\n",
    "        current = sub\n",
    "        r.append(current)\n",
    "    return r\n",
    "\n",
    "def build_pyramid(ima,levelmax):\n",
    "    \"\"\"return a list of subsampled images (using gaussion pre-filter\n",
    "    \"\"\"\n",
    "    r = [ima]\n",
    "    current = ima\n",
    "    for level in range(levelmax):\n",
    "        sub = current[::2,::2]\n",
    "        current = sub\n",
    "        r.append(current)\n",
    "    return r\n",
    "\n",
    "im = camera()[::2,::2]\n",
    "\n",
    "#build filtered and non-filtered pyramids\n",
    "N = 4\n",
    "fpyramid = build_gaussian_pyramid(im,N)\n",
    "nfpyramid = build_pyramid(im,N)\n",
    "\n",
    "fig, ax = plt.subplots(N+1,2,figsize=[10,25])\n",
    "                       \n",
    "for i,(f,nf) in enumerate(zip(fpyramid,nfpyramid)):\n",
    "    ax[i][0].imshow(f,cmap=plt.cm.gray,interpolation='nearest')\n",
    "    ax[i][0].set_title('guaussian pyramid')\n",
    "    ax[i][1].imshow(nf,cmap=plt.cm.gray,interpolation='nearest')\n",
    "    ax[i][1].set_title('subsampling pyramid');\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of oriented gradient (HoG)\n",
    "\n",
    "In order to differentiate each detected corner, one can compute local descriptors. Local gradient orientation distribution can be used.\n",
    "\n",
    "> Gradient direction can be computed from local image $x$ and $y$ derivatives\n",
    ">\n",
    "\\begin{equation}\n",
    "I_x = \\begin{bmatrix} \n",
    "+1 & 0 & -1 \\\\\n",
    "\\end{bmatrix} * \\mathbf{I}\n",
    "\\end{equation}\n",
    ">\n",
    "\\begin{equation}\n",
    "\\mbox{and} \\quad \\quad I_y = \\begin{bmatrix} \n",
    "+1 \\\\\n",
    "0  \\\\\n",
    "-1 \n",
    "\\end{bmatrix} * \\mathbf{I}\n",
    "\\end{equation}\n",
    ">\n",
    "> then gradient is \n",
    "\\begin{align}\n",
    "\\nabla I(x, y) =\n",
    "\\left(I_x, I_y\\right)\\\\\n",
    "\\end{align}\n",
    ">\n",
    "> Amplitude and angle:\n",
    "\\begin{align}\n",
    "amplitude &= \\sqrt { I_x^2 + I_y^2 }\\\\\n",
    "angle &= \\tan^{-1}( \\frac{I_x}{I_y})\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_arrows(z,ax,sub=1):\n",
    "    \n",
    "    m,n = z.shape\n",
    "    x,y = np.meshgrid(range(0,m,sub),range(0,n,sub))\n",
    "    w = np.array([-1,0,+1])\n",
    "\n",
    "    dx = convolve1d(z,w,axis=1,origin=0)\n",
    "    dy = convolve1d(z,w,axis=0,origin=0)\n",
    "\n",
    "    ax.imshow(z,alpha=.1)\n",
    "    ax.quiver(x,y,dx[::sub,::sub],dy[::sub,::sub],angles='xy',scale_units='xy', scale=.5, pivot='mid' )\n",
    "\n",
    "    \n",
    "    \n",
    "z = ima.astype(np.int8)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_arrows(z,ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = data.camera()/10.\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_arrows(z,ax,sub=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the local distribution of the gradient direction is a good signature to describe a point of interest.\n",
    "\n",
    "Hereafter the detection of the SIFT corners (position/scale and orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data.astronaut()\n",
    "sift = cv2.SIFT_create()\n",
    "kp = sift.detect(g,None)\n",
    "img=cv2.drawKeypoints(g,kp,g,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imsave('astro_points.png',img)\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SURF (Speeded Up Robust Features)\n",
    "\n",
    "speed up of the second derivative of the gaussian by using the integral images\n",
    "\n",
    "\n",
    "> Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, \"SURF: Speeded Up Robust Features\", Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integral images\n",
    "\n",
    "In order to speed up computation some approximation can be done. One very intersting algorithm is the integral image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ima = np.zeros((50,50),dtype=np.uint8)\n",
    "ima[20:30,20:30] = 1\n",
    "ima[40:45,10:15] = 1\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(ima)\n",
    "integral_image = cv2.integral(ima)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(integral_image,cmap=plt.cm.hot)\n",
    "\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Grab some test data.\n",
    "Z = integral_image\n",
    "m,n = Z.shape\n",
    "X, Y = np.mgrid[0:m,0:n]\n",
    "\n",
    "# Plot a basic wireframe.\n",
    "ax.plot_wireframe(X, Y, Z, rstride=1, cstride=1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class quad():\n",
    "    def __init__(self,xy,w,h):\n",
    "        self.xy = xy\n",
    "        self.w = w \n",
    "        self.h = h\n",
    "        self.pos_a = (xy[1], xy[0])\n",
    "        self.pos_b = (xy[1] + h, xy[0])\n",
    "        self.pos_c = (xy[1] + h, xy[0] + w)\n",
    "        self.pos_d = (xy[1], xy[0] + w)\n",
    "\n",
    "    def sample(self,image):\n",
    "        a = image[self.pos_a]\n",
    "        b = image[self.pos_b]\n",
    "        c = image[self.pos_c]\n",
    "        d = image[self.pos_d]\n",
    "        return a+c-d-b,(a,b,c,d)\n",
    "    \n",
    "    def plot(self,ax,c='r',txt=True):\n",
    "        rect = Rectangle(self.xy,self.w,self.h,facecolor=c, alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        if txt:\n",
    "            for s,p in zip(['a','b','c','d'],[self.pos_a,self.pos_b,self.pos_c,self.pos_d]):\n",
    "                ax.text(p[1],p[0],s,c='y')\n",
    "        \n",
    "q = quad((15,15),10,10)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.imshow(ima)\n",
    "q.plot(ax)\n",
    "s,abcd = q.sample(integral_image)\n",
    "print(f'integral = {s}, abcd = {abcd}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = quad((15,15),4,4)\n",
    "q2 = quad((15,19),4,4)\n",
    "q3 = quad((15,23),4,4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(ima)\n",
    "q1.plot(ax,txt=False)\n",
    "q2.plot(ax,c='y',txt=False)\n",
    "q3.plot(ax,txt=False)\n",
    "s1,_ = q1.sample(integral_image)\n",
    "s2,_ = q2.sample(integral_image)\n",
    "print(f's1: {s1}, s2: {s2}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = quad((5,5),2,2)\n",
    "q2 = quad((5,7),2,2)\n",
    "q3 = quad((5,9),2,2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(ima)\n",
    "q1.plot(ax,txt=False)\n",
    "q2.plot(ax,c='y',txt=False)\n",
    "q3.plot(ax,txt=False)\n",
    "s1,_ = q1.sample(integral_image)\n",
    "s2,_ = q2.sample(integral_image)\n",
    "print(f's1: {s1}, s2: {s2}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = quad((5,5),2,2)\n",
    "q2 = quad((5,7),2,2)\n",
    "q3 = quad((7,7),2,2)\n",
    "q4 = quad((7,5),2,2)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.imshow(ima)\n",
    "q1.plot(ax,txt=False)\n",
    "q2.plot(ax,c='y',txt=False)\n",
    "q3.plot(ax,txt=False)\n",
    "q4.plot(ax,c='y',txt=False)\n",
    "\n",
    "s1,_ = q1.sample(integral_image)\n",
    "s2,_ = q2.sample(integral_image)\n",
    "print(f's1: {s1}, s2: {s2}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/4/47/FAST_Corner_Detector.jpg\">\n",
    "\n",
    "A pixel $p$ is considered as a corner if one of the two folllowing conditions is met:\n",
    "\n",
    "* A set of $N$ contiguous pixels $S$, $\\forall x \\in S$, the intensity of $x$,  $I(x) >  I(p) + t$ \n",
    "\n",
    "* A set of $N$ contiguous pixels $S$, $\\forall x \\in S$, $I(x) < I(p) - t$\n",
    "\n",
    "> Rosten, E., and T. Drummond. 2005. “Fusing points and lines for high performance tracking.”P. 1508–1515 in Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, vol. 2. IEEE http://ieeexplore.ieee.org/xpls/ abs_all.jsp?arnumber=1544896 (Accessed February 8, 2011).\n",
    ">\n",
    "> Rosten, Edward, and Tom Drummond. 2006. “Machine learning for high-speed corner detection.” Computer Vision–ECCV 2006 430–443. http://www.springerlink.com/index/y11g42n05q626127.pdf (Accessed February 8, 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application : image warping\n",
    "\n",
    "from OpenCV examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.feature import (corner_harris, corner_subpix, corner_peaks,\n",
    "                             plot_matches)\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import ransac\n",
    "\n",
    "\n",
    "# generate synthetic checkerboard image and add gradient for the later matching\n",
    "checkerboard = img_as_float(data.checkerboard())\n",
    "img_orig = np.zeros(list(checkerboard.shape) + [3])\n",
    "img_orig[..., 0] = checkerboard\n",
    "gradient_r, gradient_c = (np.mgrid[0:img_orig.shape[0],\n",
    "                                   0:img_orig.shape[1]]\n",
    "                          / float(img_orig.shape[0]))\n",
    "img_orig[..., 1] = gradient_r\n",
    "img_orig[..., 2] = gradient_c\n",
    "img_orig = rescale_intensity(img_orig)\n",
    "img_orig_gray = rgb2gray(img_orig)\n",
    "\n",
    "# warp synthetic image\n",
    "tform = AffineTransform(scale=(0.9, 0.9), rotation=0.2, translation=(20, -10))\n",
    "img_warped = warp(img_orig, tform.inverse, output_shape=(200, 200))\n",
    "img_warped_gray = rgb2gray(img_warped)\n",
    "\n",
    "# extract corners using Harris' corner measure\n",
    "coords_orig = corner_peaks(corner_harris(img_orig_gray), threshold_rel=0.001,\n",
    "                           min_distance=5)\n",
    "coords_warped = corner_peaks(corner_harris(img_warped_gray),\n",
    "                             threshold_rel=0.001, min_distance=5)\n",
    "\n",
    "# determine sub-pixel corner position\n",
    "coords_orig_subpix = corner_subpix(img_orig_gray, coords_orig, window_size=9)\n",
    "coords_warped_subpix = corner_subpix(img_warped_gray, coords_warped,\n",
    "                                     window_size=9)\n",
    "\n",
    "\n",
    "def gaussian_weights(window_ext, sigma=1):\n",
    "    y, x = np.mgrid[-window_ext:window_ext+1, -window_ext:window_ext+1]\n",
    "    g = np.zeros(y.shape, dtype=np.double)\n",
    "    g[:] = np.exp(-0.5 * (x**2 / sigma**2 + y**2 / sigma**2))\n",
    "    g /= 2 * np.pi * sigma * sigma\n",
    "    return g\n",
    "\n",
    "\n",
    "def match_corner(coord, window_ext=5):\n",
    "    r, c = np.round(coord).astype(np.intp)\n",
    "    window_orig = img_orig[r-window_ext:r+window_ext+1,\n",
    "                           c-window_ext:c+window_ext+1, :]\n",
    "\n",
    "    # weight pixels depending on distance to center pixel\n",
    "    weights = gaussian_weights(window_ext, 3)\n",
    "    weights = np.dstack((weights, weights, weights))\n",
    "\n",
    "    # compute sum of squared differences to all corners in warped image\n",
    "    SSDs = []\n",
    "    for cr, cc in coords_warped:\n",
    "        window_warped = img_warped[cr-window_ext:cr+window_ext+1,\n",
    "                                   cc-window_ext:cc+window_ext+1, :]\n",
    "        SSD = np.sum(weights * (window_orig - window_warped)**2)\n",
    "        SSDs.append(SSD)\n",
    "\n",
    "    # use corner with minimum SSD as correspondence\n",
    "    min_idx = np.argmin(SSDs)\n",
    "    return coords_warped_subpix[min_idx]\n",
    "\n",
    "\n",
    "# find correspondences using simple weighted sum of squared differences\n",
    "src = []\n",
    "dst = []\n",
    "for coord in coords_orig_subpix:\n",
    "    src.append(coord)\n",
    "    dst.append(match_corner(coord))\n",
    "src = np.array(src)\n",
    "dst = np.array(dst)\n",
    "\n",
    "\n",
    "# estimate affine transform model using all coordinates\n",
    "model = AffineTransform()\n",
    "model.estimate(src, dst)\n",
    "\n",
    "# robustly estimate affine transform model with RANSAC\n",
    "model_robust, inliers = ransac((src, dst), AffineTransform, min_samples=3,\n",
    "                               residual_threshold=2, max_trials=100)\n",
    "outliers = inliers == False\n",
    "\n",
    "\n",
    "# compare \"true\" and estimated transform parameters\n",
    "print(\"Ground truth:\")\n",
    "print(f'Scale: ({tform.scale[1]:.4f}, {tform.scale[0]:.4f}), '\n",
    "      f'Translation: ({tform.translation[1]:.4f}, '\n",
    "      f'{tform.translation[0]:.4f}), '\n",
    "      f'Rotation: {-tform.rotation:.4f}')\n",
    "print(\"Affine transform:\")\n",
    "print(f'Scale: ({model.scale[0]:.4f}, {model.scale[1]:.4f}), '\n",
    "      f'Translation: ({model.translation[0]:.4f}, '\n",
    "      f'{model.translation[1]:.4f}), '\n",
    "      f'Rotation: {model.rotation:.4f}')\n",
    "print(\"RANSAC:\")\n",
    "print(f'Scale: ({model_robust.scale[0]:.4f}, {model_robust.scale[1]:.4f}), '\n",
    "      f'Translation: ({model_robust.translation[0]:.4f}, '\n",
    "      f'{model_robust.translation[1]:.4f}), '\n",
    "      f'Rotation: {model_robust.rotation:.4f}')\n",
    "\n",
    "# visualize correspondence\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1,figsize=[10,10])\n",
    "\n",
    "plt.gray()\n",
    "\n",
    "inlier_idxs = np.nonzero(inliers)[0]\n",
    "plot_matches(ax[0], img_orig_gray, img_warped_gray, src, dst,\n",
    "             np.column_stack((inlier_idxs, inlier_idxs)), matches_color='b')\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('Correct correspondences')\n",
    "\n",
    "outlier_idxs = np.nonzero(outliers)[0]\n",
    "plot_matches(ax[1], img_orig_gray, img_warped_gray, src, dst,\n",
    "             np.column_stack((outlier_idxs, outlier_idxs)), matches_color='r')\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('Faulty correspondences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "demo/webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
